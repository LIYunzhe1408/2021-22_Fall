# 爬虫的理解和应用

李昀哲 Yunzhe Li @ college of computer engineering and science, Shanghai University

爬虫是模拟浏览器发送请求，一种按照一定的规则，自动地抓取互联网信息的程序。

信息的获取相对于过去信息面变广了，但无用的信息也变多了，因此筛选是相当重要的。

对于当下前沿的机器学习来说，需要将知识转化为机器能懂的符号语言来进行训练，但知识如上所说，是存在冗余的，真正有价值的数据集是很有限的，那么如何精确、批量的获取我们需要的数据集，就成了机器学习领域除算法外另一个关键点。

“爬虫”就是这样一种技术，在大量数据中根据我们指定的要求，分析删选出真正需要的数据。



前期准备：阅读刘悦老师提供的python爬虫代码，并利用代码实现简单的对Web of science网站上关于machine learning和material的爬虫，并存入本地文件中。



依赖的库：

```python
from bs4 import BeautifulSoup #网页解析，获取数据
import re #正则表达式，进行文字匹配
import urllib.request, urllib.error #制定url，获取网页数据
import xlwt #进行excel操作
```



基本流程：

爬取网页，注意解析数据，保存网页





### 简单的爬虫实现：

###### 爬虫的爬取数据的步骤

1.确定需要爬取的url地址
2.由请求模块向url发起请求，并得到网站回应
3.从相应内容中提取数据：
1.所需数据保存
2.页面中有其他跟进的url，继续第二步去发起请求，如此循环。

```python
import urllib.request

#向url发送请求返回对象，构建response类
response = urllib.request.urlopen('http://www.baidu.com/')

#response类的read()方法，提取响应内容
html = response.read().decode('utf-8')

#显示响应内容
print(html)

```



###### 常用的方法

1. urllib库用以实现request的发送

   - urllib.request.urlopen()
     - 向网站发出请求并获得返回对象
     - urllib.request提供最基本的构造HTTP请求的方法，模拟浏览器的一个请求发起
     - parameter：网址，等待超时时间

2. 响应对象的方法

   ```python
   html = response.read().decode() #没有.decode()得到的是bytes类型
   url = response.geturl()
   code = response.getcode() #返回http响应的状态码
   ```



###### 网站对于爬虫和人类访问的判断

请求头User-Agent就是网站识别我们的信息。

浏览器访问时的User-Agent为：

```http
"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36 Edg/95.0.1020.30", 
```

使用python访问：

```python
import urllib.request

url = 'http://httpbin.org/get'
response = request.urlopen(url)
html = response.read().decode('utf8')
print(html)
```

得到的User-Agent：

```http
 "User-Agent": "Python-urllib/3.7", 
```

由上述可见，通过对“请求头”的辨别，很容易区分人类和爬虫，因此需要对User Agent进行重构

###### 重构User-Agent

```python
import urllib.request

url = 'http://httpbin.org/get'
#定义新的User_Agent
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko)'
                         ' Chrome/14.0.835.163 Safari/535.1'}
#Request函数重构“头”
req = urllib.request.Request(url=url, headers=headers)
#再次发送请求
res = urllib.request.urlopen(req)
html = res.read().decode()
print(html)
```

得到的User-Agent：

```http
"User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.835.163 Safari/535.1"
```



###### url编码模块

百度“上海大学”，得到冗长url

```
https://www.baidu.com/s?wd=%E4%B8%8A%E6%B5%B7%E5%A4%A7%E5%AD%A6&rsv_spt=1&rsv_iqid=0xf4033ad4000ac47f&issp=1&f=8&rsv_bp=1&rsv_idx=2&ie=utf-8&tn=15007414_8_dg&rsv_enter=1&rsv_dl=tb&rsv_sug3=12&rsv_sug1=18&rsv_sug7=101&rsv_sug2=0&rsv_btype=i&inputT=5207&rsv_sug4=5207
```

其中只有如下部分是有用的：

```
#wd之后的是上海大学的编码，汉字是需要编码的
https://www.baidu.com/s?wd=%E4%B8%8A%E6%B5%B7%E5%A4%A7%E5%AD%A6
```

方法及常用参数：

```python
import urllib.parse
from urllib import parse

#查询参数{'wd': '上海大学'}
query_string = {'wd': '上海大学'}
result = urllib.parse.urlencode(query_string)
print(result)
#wd=%E4%B8%8A%E6%B5%B7%E5%A4%A7%E5%AD%A6
```

多个参数时

```python
#查询多个参数
#百度一个页面显示10个信息（除广告外），pn=10表示第二页
param = {'wd': '上海大学', 'pn': '10'}
result = parse.urlencode(param)
print(result)
#wd=%E4%B8%8A%E6%B5%B7%E5%A4%A7%E5%AD%A6&pn=10
```

由上：可以通过字符串拼接得到需要的url：（得到百度搜索上海大学的第二页）

```python
url = 'http://baidu.com/s?{}'.format(result)
print(url)
#http://baidu.com/s?wd=%E4%B8%8A%E6%B5%B7%E5%A4%A7%E5%AD%A6&pn=10
```

###### 拼接URL的三种方式：

先得到参数编码：

```python
params = {'wd':'上海大学', 'pn':'10'}
result = urllib.parse.urlencode(params)
print(result)
```

- 字符串相加：

  ```python
  baseurl = 'http://baidu.com/s?'
  #由result的得到
  param = 'wd=%E4%B8%8A%E6%B5%B7%E5%A4%A7%E5%AD%A6&pn=10'
  url = baseurl + param
  print(url)
  ```

- 字符串占位符

  ```python
  param = 'wd=%E4%B8%8A%E6%B5%B7%E5%A4%A7%E5%AD%A6&pn=10'
  url = 'http://baidu.com/s?%s'%param
  print(url)
  ```

- format：注意要在baseurl最后加{}

  ```python
  param = 'wd=%E4%B8%8A%E6%B5%B7%E5%A4%A7%E5%AD%A6&pn=10'
  baseurl = 'http://baidu.com/s?{}'
  url = baseurl.format(param)
  print(url)
  ```

  

  

  ###### 全流程实现：

  ```python
  import urllib.parse
  import urllib.request
  
  ''' 
  爬虫的爬取数据的步骤
  
  1.确定需要爬取的url地址
  2.由请求模块向url发起请求，并得到网站回应
  3.从相应内容中提取数据：
  1.所需数据保存
  2.页面中有其他跟进的url，继续第二步去发起请求，如此循环。
  '''
  
  def get_url(word):
      baseurl = 'http://baidu.com/s?'
      param = {'wd': 'word'}
      result_param = urllib.parse.urlencode(param)
      url = baseurl + result_param
      return url
  
  def send_request(url):
      headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko)'
                               ' Chrome/14.0.835.163 Safari/535.1'}
      req = urllib.request.Request(url=url, headers=headers)
      res = urllib.request.urlopen(req)
      html = res.read().decode()
      return html
  
  def save(word, html):
      filename = word + '.html'
      with open(filename, 'w', encoding='utf-8') as f:
          f.write(html)
  
  if __name__ == '__main__':
      word = input('Search: ')
      url = get_url(word)
      html = send_request(url)
      save(word, html)
  ```

###### 一些小结：

```python
urllib.request
req=request.Request(url=url,headers=headers)
res=request.urlopen(req)
html=res.read().decode()
#响应对象方法
res.read()
res.getcode()
res.geturl()
#urllib.parse
parse.urlencode({params})
parse.quote() #用于解码
parse.unquote()

网页参考：https://blog.csdn.net/qq_48322394/article/details/114014358?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163540693716780264047726%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=163540693716780264047726&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-4-114014358.pc_search_result_cache&utm_term=Python%E7%88%AC%E8%99%AB%E8%B6%85%E8%AF%A6%E7%BB%86%E8%AE%B2%E8%A7%A3&spm=1018.2226.3001.4187
```

